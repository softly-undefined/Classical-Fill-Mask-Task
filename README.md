# Classical Chinese Fill-Mask Task

This project is an exploration of how well the SikuBERT and SikuRoBERTa models created by Nanjing Agricultural University performs on a simple fill-mask task compared to other models (specifically a bilingual BERT and RoBERTa, gpt-4o and gpt-4o-mini, and claude opus and haiku).

The text used is a story from Yueweicaotangbiji. Findings can be found in the analysis.ipynb file, and the code used to generate the predictions is available in model.py

SikubERT: https://huggingface.co/SIKU-BERT/sikubert

SikuRoBERTa: https://huggingface.co/SIKU-BERT/sikuroberta
